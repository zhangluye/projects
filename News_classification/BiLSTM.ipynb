{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ===============================  data loading success! ===================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "#读取第一列、第二列、第四列\n",
    "\n",
    "traindata_path = './data_processed/train_data.csv'\n",
    "df_all = pd.read_csv(traindata_path, header=None, engine=\"python\", encoding=\"utf-8\")  #8597\n",
    "df_all.columns = [\"index\",\"words\",\"types\",\"caijing\",\"fangchan\",\"jiaoyu\",\"junshi\",\"keji\",\"qiche\",\"tiyu\",\"youxi\",\"yule\"]\n",
    "df_all.drop(0,inplace=True)\n",
    "df_all.drop(['index'],inplace=True,axis=1)\n",
    "df_all = df_all.sample(frac = 1).reset_index(drop=True)\n",
    "\n",
    "df_all.reset_index(drop=True)\n",
    "print(' ===============================  data loading success! ===================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_types = df_all[['types']]\n",
    "df_types = df_types.replace('caijing', 0)\n",
    "df_types = df_types.replace('fangchan', 1)\n",
    "df_types =df_types.replace('jiaoyu', 2)\n",
    "df_types =df_types.replace('junshi', 3)\n",
    "df_types =df_types.replace('keji', 4)\n",
    "df_types =df_types.replace('qiche', 5)\n",
    "df_types =df_types.replace('tiyu', 6)\n",
    "df_types =df_types.replace('youxi', 7)\n",
    "df_types =df_types.replace('yule', 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import *\n",
    "\n",
    "x_series = df_all['words']\n",
    "labels = to_categorical(df_types['types'].values)\n",
    "\n",
    "#将数据集按9:1分为训练集和测试集\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_series, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: 1137\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 500\n",
    "\n",
    "#创建一个Tokenizer对象，fit_on_texts函数可以将输入的文本中的每个词编号，编号是根据词频的，词频越大，编号越小\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_series)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# 将每个样本中的每个词转换为数字列表，使用每个词的编号进行编号\n",
    "x_train_word_ids = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_word_ids = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "# 每条样本长度不唯一，将每条样本的长度设置一个固定值\n",
    "x_train_padded_seqs=pad_sequences(x_train_word_ids,maxlen) #将超过固定值的部分截掉，不足的在最前面用0填充\n",
    "x_test_padded_seqs=pad_sequences(x_test_word_ids, maxlen)\n",
    "\n",
    "print('Shape of data tensor:', len(x_train_word_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyz/anaconda3/lib/python3.6/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "# 加载bin格式的模型\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(\"./model/content_w2v.bin\", binary=True)\n",
    "\n",
    "# 预训练的词向量中没有出现的词用0向量表示，出现的词在w2v模型中找到其对应的向量\n",
    "embedding_matrix = np.zeros((len(word_index) + 1 , 100))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embedding_vector = w2v_model[str(word)]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "46/46 [==============================] - 16s 291ms/step - loss: 0.6963 - accuracy: 0.4531 - val_loss: 0.5449 - val_accuracy: 0.6886\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 12s 265ms/step - loss: 0.4467 - accuracy: 0.8102 - val_loss: 0.4058 - val_accuracy: 0.7500\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 12s 265ms/step - loss: 0.3356 - accuracy: 0.8449 - val_loss: 0.2808 - val_accuracy: 0.8596\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 12s 262ms/step - loss: 0.2239 - accuracy: 0.8806 - val_loss: 0.1816 - val_accuracy: 0.8947\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 12s 261ms/step - loss: 0.1436 - accuracy: 0.9205 - val_loss: 0.1358 - val_accuracy: 0.8465\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 12s 262ms/step - loss: 0.1029 - accuracy: 0.9383 - val_loss: 0.1146 - val_accuracy: 0.8509\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 12s 262ms/step - loss: 0.0852 - accuracy: 0.9160 - val_loss: 0.0947 - val_accuracy: 0.8860\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 12s 261ms/step - loss: 0.0671 - accuracy: 0.9364 - val_loss: 0.0861 - val_accuracy: 0.9123\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 12s 264ms/step - loss: 0.0507 - accuracy: 0.9559 - val_loss: 0.0896 - val_accuracy: 0.8728\n",
      "Epoch 10/50\n",
      "46/46 [==============================] - 12s 263ms/step - loss: 0.0605 - accuracy: 0.9373 - val_loss: 0.0706 - val_accuracy: 0.9254\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 12s 261ms/step - loss: 0.0398 - accuracy: 0.9648 - val_loss: 0.0716 - val_accuracy: 0.8991\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 12s 262ms/step - loss: 0.0342 - accuracy: 0.9720 - val_loss: 0.0691 - val_accuracy: 0.9035\n",
      "Epoch 13/50\n",
      "46/46 [==============================] - 12s 261ms/step - loss: 0.0254 - accuracy: 0.9847 - val_loss: 0.1080 - val_accuracy: 0.8596\n",
      "Epoch 14/50\n",
      "46/46 [==============================] - 12s 262ms/step - loss: 0.0362 - accuracy: 0.9677 - val_loss: 0.0708 - val_accuracy: 0.9254\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 12s 261ms/step - loss: 0.0279 - accuracy: 0.9814 - val_loss: 0.0716 - val_accuracy: 0.9123\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 12s 261ms/step - loss: 0.0200 - accuracy: 0.9872 - val_loss: 0.0625 - val_accuracy: 0.9342\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 12s 262ms/step - loss: 0.0150 - accuracy: 0.9937 - val_loss: 0.0580 - val_accuracy: 0.9474\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 12s 265ms/step - loss: 0.0140 - accuracy: 0.9941 - val_loss: 0.0832 - val_accuracy: 0.8904\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 12s 262ms/step - loss: 0.0153 - accuracy: 0.9924 - val_loss: 0.0652 - val_accuracy: 0.9254\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 12s 266ms/step - loss: 0.0129 - accuracy: 0.9938 - val_loss: 0.0630 - val_accuracy: 0.9342\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 12s 264ms/step - loss: 0.0090 - accuracy: 0.9991 - val_loss: 0.0721 - val_accuracy: 0.9211\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 12s 264ms/step - loss: 0.0114 - accuracy: 0.9932 - val_loss: 0.0788 - val_accuracy: 0.8991\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 12s 261ms/step - loss: 0.0131 - accuracy: 0.9907 - val_loss: 0.0834 - val_accuracy: 0.9123\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 12s 264ms/step - loss: 0.0074 - accuracy: 0.9969 - val_loss: 0.0723 - val_accuracy: 0.9211\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 12s 265ms/step - loss: 0.0080 - accuracy: 0.9981 - val_loss: 0.0973 - val_accuracy: 0.9167\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 12s 264ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0673 - val_accuracy: 0.9386\n",
      "Epoch 27/50\n",
      "46/46 [==============================] - 12s 263ms/step - loss: 0.0054 - accuracy: 0.9976 - val_loss: 0.0855 - val_accuracy: 0.9123\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 12s 264ms/step - loss: 0.0110 - accuracy: 0.9923 - val_loss: 0.1058 - val_accuracy: 0.8640\n",
      "Epoch 29/50\n",
      "46/46 [==============================] - 12s 262ms/step - loss: 0.0158 - accuracy: 0.9865 - val_loss: 0.1081 - val_accuracy: 0.8991\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 12s 263ms/step - loss: 0.0091 - accuracy: 0.9939 - val_loss: 0.0891 - val_accuracy: 0.8991\n",
      "Epoch 31/50\n",
      "46/46 [==============================] - 12s 264ms/step - loss: 0.0062 - accuracy: 0.9953 - val_loss: 0.1117 - val_accuracy: 0.8860\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 12s 262ms/step - loss: 0.0103 - accuracy: 0.9893 - val_loss: 0.0809 - val_accuracy: 0.9079\n",
      "Epoch 33/50\n",
      "46/46 [==============================] - 12s 268ms/step - loss: 0.0044 - accuracy: 0.9999 - val_loss: 0.0998 - val_accuracy: 0.9211\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 12s 265ms/step - loss: 0.0084 - accuracy: 0.9928 - val_loss: 0.0894 - val_accuracy: 0.9254\n",
      "Epoch 35/50\n",
      "46/46 [==============================] - 12s 264ms/step - loss: 0.0056 - accuracy: 0.9964 - val_loss: 0.0815 - val_accuracy: 0.9386\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 12s 265ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0843 - val_accuracy: 0.9211\n",
      "Epoch 37/50\n",
      "46/46 [==============================] - 12s 264ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0927 - val_accuracy: 0.9298\n",
      "Epoch 38/50\n",
      "46/46 [==============================] - 12s 263ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0834 - val_accuracy: 0.9386\n",
      "Epoch 39/50\n",
      "46/46 [==============================] - 12s 265ms/step - loss: 0.0028 - accuracy: 0.9972 - val_loss: 0.0904 - val_accuracy: 0.9342\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 12s 264ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9342\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 12s 264ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0933 - val_accuracy: 0.9342\n",
      "Epoch 42/50\n",
      "46/46 [==============================] - 12s 265ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0934 - val_accuracy: 0.9298\n",
      "Epoch 43/50\n",
      "46/46 [==============================] - 12s 264ms/step - loss: 0.0020 - accuracy: 0.9998 - val_loss: 0.0971 - val_accuracy: 0.9254\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 12s 262ms/step - loss: 0.0046 - accuracy: 0.9953 - val_loss: 0.1353 - val_accuracy: 0.8728\n",
      "Epoch 45/50\n",
      "46/46 [==============================] - 12s 262ms/step - loss: 0.0098 - accuracy: 0.9881 - val_loss: 0.0938 - val_accuracy: 0.9298\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 12s 261ms/step - loss: 0.0064 - accuracy: 0.9983 - val_loss: 0.1014 - val_accuracy: 0.8904\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 12s 264ms/step - loss: 0.0092 - accuracy: 0.9895 - val_loss: 0.0961 - val_accuracy: 0.9079\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 12s 265ms/step - loss: 0.0023 - accuracy: 0.9999 - val_loss: 0.0909 - val_accuracy: 0.9123\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 12s 264ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0804 - val_accuracy: 0.9298\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 12s 265ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0854 - val_accuracy: 0.9254\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input,Embedding,Bidirectional,LSTM,BatchNormalization,Dense,Dropout,Lambda\n",
    "from tensorflow.keras import Sequential\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, 100, input_length=maxlen,weights=[embedding_matrix],trainable=False))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(labels.shape[1],activation=\"softmax\")) \n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=\"adam\",\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(x_train_padded_seqs, y_train, epochs=50, batch_size=20,validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_f1(y_predict,y_test):\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1=[]\n",
    "    for label in range(y_predict.shape[1]):\n",
    "        tp=0\n",
    "        fp=0\n",
    "        tn=0\n",
    "        fn=0\n",
    "        for i in range(len(y_test)):\n",
    "            if np.argmax(y_test[i])==label:\n",
    "                if (np.argmax(y_test[i]))==(np.argmax(y_predict[i])):\n",
    "                    tp+=1\n",
    "                else:\n",
    "                    fn+=1\n",
    "            else:\n",
    "                if (np.argmax(y_predict[i]))!=label:\n",
    "                    tn+=1\n",
    "                else:\n",
    "                    fp+=1\n",
    "        precision=tp/(tp+fp)\n",
    "        recall=tp/(tp+fn)\n",
    "        \n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        \n",
    "        f1.append(2*precision*recall/(precision+recall))\n",
    "    return precision_list,recall_list,f1\n",
    "        \n",
    "def get_avg(list1):\n",
    "    avg = 0\n",
    "    for i in range(len(list1)):\n",
    "        avg += list1[i]\n",
    "    return avg/len(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average_precision: 0.9326277114816668\n",
      "Average_recall: 0.925288473588282\n",
      "Average_f1: 0.9267596009763016\n"
     ]
    }
   ],
   "source": [
    "y_predict =  model.predict(x_test_padded_seqs)\n",
    "precision_list,recall_list,f1_list = return_f1(y_predict,y_test)\n",
    "print('Average_precision:',get_avg(precision_list))\n",
    "print('Average_recall:',get_avg(recall_list))\n",
    "print('Average_f1:',get_avg(f1_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       model name: BiLSTM\n",
      "     precision   recall   f1-score\n",
      "财经:   0.864     0.704     0.776\n",
      "房产:   0.857     0.909     0.882\n",
      "教育:   0.966     1.000     0.982\n",
      "军事:   0.912     0.969     0.939\n",
      "科技:   1.000     0.862     0.926\n",
      "汽车:   0.825     0.943     0.880\n",
      "体育:   1.000     0.971     0.986\n",
      "游戏:   1.000     0.970     0.985\n",
      "娱乐:   0.971     1.000     0.985\n",
      "平均值: 0.933     0.925     0.927\n"
     ]
    }
   ],
   "source": [
    "print('       model name: BiLSTM')\n",
    "print('     precision   recall   f1-score')\n",
    "print('财经:   %.3f'%precision_list[0],'    %.3f'%recall_list[0],'    %.3f'%f1_list[0])\n",
    "print('房产:   %.3f'%precision_list[1],'    %.3f'%recall_list[1],'    %.3f'%f1_list[1])\n",
    "print('教育:   %.3f'%precision_list[2],'    %.3f'%recall_list[2],'    %.3f'%f1_list[2])\n",
    "print('军事:   %.3f'%precision_list[3],'    %.3f'%recall_list[3],'    %.3f'%f1_list[3])\n",
    "print('科技:   %.3f'%precision_list[4],'    %.3f'%recall_list[4],'    %.3f'%f1_list[4])\n",
    "print('汽车:   %.3f'%precision_list[5],'    %.3f'%recall_list[5],'    %.3f'%f1_list[5])\n",
    "print('体育:   %.3f'%precision_list[6],'    %.3f'%recall_list[6],'    %.3f'%f1_list[6])\n",
    "print('游戏:   %.3f'%precision_list[7],'    %.3f'%recall_list[7],'    %.3f'%f1_list[7])\n",
    "print('娱乐:   %.3f'%precision_list[8],'    %.3f'%recall_list[8],'    %.3f'%f1_list[8])\n",
    "print('平均值: %.3f'%get_avg(precision_list),'    %.3f'%get_avg(recall_list),'    %.3f'%get_avg(f1_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
